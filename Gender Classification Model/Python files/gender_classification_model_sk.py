# -*- coding: utf-8 -*-
"""Gender Classification Model_SK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DhFadDSoOkHH8bnv94aXkYATCrPrZTuv

#**To categorize a user's gender we are going to deploy a classification model .**

#**Importing Necessary Libraries**
"""

!pip install sentence-transformers

import numpy as np
import pandas as pd
import seaborn as sns
import random
from google.colab import drive
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,classification_report, precision_recall_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

"""

#**Data Loading**"""

from google.colab import drive
drive.mount('/content/drive')

flight=pd.read_csv("/content/drive/MyDrive/travel_capstone/flights.csv",on_bad_lines='skip')
hotel=pd.read_csv("/content/drive/MyDrive/travel_capstone/hotels.csv",on_bad_lines='skip')
user=pd.read_csv("/content/drive/MyDrive/travel_capstone/users.csv",on_bad_lines='skip')

"""## Dataset First view"""

flight.head()

hotel.head()

user.head()

"""#**Data Preprocessing**

### Dataset Rows & Columns count
"""

user.shape

"""### Checking Null Values for User Dataset


"""

#check for missing values
user.isnull().sum()

#check for duplicate rows
user.duplicated().sum()

#check for data types of different features
user.info()

"""**Summary:-**
1. The gender feature, our dependent variable, will be encoded using a label encoder to convert its categorical values (i.e., male and female) into numeric representations suitable for the classification model. Here, 1 will denote male, and 0 will denote female.

2. The company feature holds a non-ordinal relationship, meaning the categories do not have a meaningful order. Therefore, using One-Hot Encoding is the best option to transform its categorical values into a numeric format suitable for the model.

3. For the name feature, we will apply an NLP model, such as a sentence transformer, to extract meaningful embeddings. Additionally, we will use PCA (Principal Component Analysis) to reduce dimensionality before using it in the classification model.
"""

# 5 Point Summary of numerical features
user.describe()

user.describe(include=['object', 'category'])

user['company'].value_counts()

user.sample(n=5)

user[user['name']=='Charlotte Johnson']

# Summary of categorical features
user.describe(include=object)

"""1. As we see ,we have 3 categories in our target variable ,let's deep dive into the target variable.

"""

user['gender'].value_counts()

#filtering records based on relavent categories in the target variable
user_df=user[(user['gender']=='male') | (user['gender']=='female') ]

from sklearn.preprocessing import OneHotEncoder
import pandas as pd
one_hot_encoder = OneHotEncoder()

# One-Hot Encode the 'company' feature
company_encoded = one_hot_encoder.fit_transform(user_df[['company']]).toarray()

# Add one-hot encoded columns back to the DataFrame
company_encoded_df = pd.DataFrame(company_encoded, columns=one_hot_encoder.get_feature_names_out(['company']))
user_df = pd.concat([user_df, company_encoded_df], axis=1)

# Encode the 'gender' feature using Label Encoding
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
user_df['gender_encoded'] = label_encoder.fit_transform(user_df['gender'])

# # Encode userCode and company to numeric values
# label_encoder = LabelEncoder()

# user_df['company_encoded'] = label_encoder.fit_transform(user_df['company'])
# user_df['gender_encoded'] = label_encoder.fit_transform(user_df['gender'])

user_df.head()

from sentence_transformers import SentenceTransformer
import pandas as pd

# Initialize the SentenceTransformer model
model = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')

# Handle missing values and ensure all are strings
text_columns = ['name']
for column in text_columns:
    user_df[column] = user_df[column].fillna('').astype(str)  # Replace NaN with empty string

# Encode text columns into embeddings
for column in text_columns:
    user_df[column + '_embedding'] = user_df[column].apply(lambda text: model.encode(text))

# Concatenate embeddings into a single feature vector (if required)
text_embeddings = user_df[[column + '_embedding' for column in text_columns]].values.tolist()

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# Update numerical features to include one-hot encoded company columns
numerical_features = ['code', 'age'] + [col for col in user_df.columns if col.startswith('company_')]

# Extract numerical features
X_numerical = user_df[numerical_features].values

# Apply PCA to text embeddings
n_components = 23  # Adjust the number of components as needed
pca = PCA(n_components=n_components)
text_columns = ['name']

# Create an empty array to store PCA-transformed embeddings
text_embeddings_pca = np.empty((len(user_df), n_components * len(text_columns)))

for i, column in enumerate(text_columns):
    embeddings = np.array(user_df[column + '_embedding'].tolist())
    embeddings_pca = pca.fit_transform(embeddings)
    text_embeddings_pca[:, i * n_components:(i + 1) * n_components] = embeddings_pca

# Combine PCA-transformed text embeddings and numerical features
X = np.hstack((text_embeddings_pca, X_numerical))

# Target variable
y = user_df['gender_encoded']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

len(text_embeddings)

print("Shape of X:", X.shape)

user_df[numerical_features].values

# Calculate missing values in percentage for training and test data
train_missing_percentage = pd.DataFrame(X_train).isnull().sum()/len(pd.DataFrame(X_train))*100
test_missing_percentage = pd.DataFrame(X_test).isnull().sum() * 100

print("Missing values in training data (percentage):\n", train_missing_percentage)
print("Missing values in test data (percentage):\n", test_missing_percentage)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')  # You can also use 'median' or 'constant'
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

"""#**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer

# Impute missing values
imputer = SimpleImputer(strategy='median')  # You can adjust the strategy if needed
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Initialize and fit the Logistic Regression model
lr_classifier = LogisticRegression(random_state=42)
lr_classifier.fit(X_train, y_train)

# Predictions
y_pred_lr = lr_classifier.predict(X_test)

# Accuracy
accuracy = lr_classifier.score(X_test, y_test)
print("Accuracy:", accuracy)

# Classification report
report = classification_report(y_test, y_pred_lr)
print("\nClassification Report:\n", report)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_lr)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.impute import SimpleImputer

# Create an imputer to fill missing values with the median
imputer = SimpleImputer(strategy='median')

# Apply the imputer to both training and validation data
X_train_imputed = imputer.fit_transform(X_train)
X_val_imputed = imputer.transform(X_val)

# Now train the model on the imputed data
lr_classifier.fit(X_train_imputed, y_train)

# Predictions on training and validation sets
y_train_pred = lr_classifier.predict(X_train_imputed)
y_val_pred = lr_classifier.predict(X_val_imputed)

# Calculate accuracy on training and validation sets
train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_val, y_val_pred)

# Print the accuracies
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Check if overfitting
if train_accuracy > val_accuracy:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

model_result = pd.DataFrame([['Logistic Regression Baseline', train_accuracy,val_accuracy]],
               columns = ['Model', 'Train accuracy', 'Validation accuracy'])

model_result

"""#**Decision Tree Classifier**"""

# Initialize a Random Forest Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Fit the model to the training data
dt_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_dt = dt_classifier.predict(X_test)

# Calculate and print accuracy
accuracy = dt_classifier.score(X_test, y_test)
print("Accuracy:", accuracy)

# Generate a classification report
report = classification_report(y_test, y_pred_dt)
print("\nClassification Report:\n", report)

cm = confusion_matrix(y_test, y_pred_dt)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model on the training set
dt_classifier.fit(X_train, y_train)

# Predictions on training and validation sets
y_train_pred = dt_classifier.predict(X_train)
y_val_pred = dt_classifier.predict(X_val)

# Calculate accuracy on training and validation sets
train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_val, y_val_pred)

# Print the accuracies
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Check if overfitting
if train_accuracy > val_accuracy:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

model = pd.DataFrame([['Decesion Tree Classifier Baseline', train_accuracy,val_accuracy]],
               columns = ['Model', 'Train accuracy', 'Validation accuracy'])
model_result = pd.concat([model_result,model],axis=0,ignore_index = True)
model_result

"""#**Random Forest Classifier**"""

# Initialize a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model to the training data
rf_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_rf = rf_classifier.predict(X_test)

"""###**Evaluation**"""

# Calculate and print accuracy
accuracy = rf_classifier.score(X_test, y_test)
print("Accuracy:", accuracy)

# Generate a classification report
report = classification_report(y_test, y_pred_rf)
print("\nClassification Report:\n", report)

cm = confusion_matrix(y_test, y_pred_rf)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model on the training set
rf_classifier.fit(X_train, y_train)

# Predictions on training and validation sets
y_train_pred = rf_classifier.predict(X_train)
y_val_pred = rf_classifier.predict(X_val)

# Calculate accuracy on training and validation sets
train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_val, y_val_pred)

# Print the accuracies
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Check if overfitting
if train_accuracy > val_accuracy:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

model = pd.DataFrame([['Random Forest Baseline', train_accuracy,val_accuracy]],
               columns = ['Model', 'Train accuracy', 'Validation accuracy'])
model_result = pd.concat([model_result,model],axis=0,ignore_index = True)
model_result

"""#**Gradient Boosting Classifier**"""

from sklearn.impute import SimpleImputer

# Initialize the imputer to fill NaN values with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Fit the imputer and transform the training and test data
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Now fit the model with the imputed data
gb_classifier.fit(X_train_imputed, y_train)

# Make predictions and evaluate
y_pred_gb = gb_classifier.predict(X_test_imputed)
accuracy = gb_classifier.score(X_test_imputed, y_test)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_gb)
print("\nClassification Report:\n", report)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_gb)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split

# Initialize the imputer to fill NaN values with the mean of the column
imputer = SimpleImputer(strategy='mean')

# Impute missing values in the training and validation sets
X_imputed = imputer.fit_transform(X)

# Split the imputed data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

# Initialize the GradientBoostingClassifier
gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Train the model on the training set
gb_classifier.fit(X_train, y_train)

# Predictions on training and validation sets
y_train_pred = gb_classifier.predict(X_train)
y_val_pred = gb_classifier.predict(X_val)

# Calculate accuracy on training and validation sets
train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_val, y_val_pred)

# Print the accuracies
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Check for overfitting
if train_accuracy > val_accuracy:
    print("The model is overfitting.")
else:
    print("The model is not overfitting.")

model = pd.DataFrame([['XGBoost Classifier Baseline', train_accuracy,val_accuracy]],
               columns = ['Model', 'Train accuracy', 'Validation accuracy'])
model_result = pd.concat([model_result,model],axis=0,ignore_index = True)
model_result

"""#**ROC-AUC Curve**"""

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import pandas as pd

# Function to train and evaluate the model
def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, model_name):
    # Fit the model to the training data
    model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = model.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Evaluate the model
    report = classification_report(y_test, y_pred)

    # Print accuracy and classification report
    #print(f"\nAccuracy for {model_name}: {accuracy:.2f}")
    #print(f"Classification Report for {model_name}:\n{report}")

    # Return accuracy, classification report, and predictions
    return accuracy, report, y_pred

# Initialize models
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
gradient_boosting = GradientBoostingClassifier(n_estimators=100, random_state=42)
decision_tree = DecisionTreeClassifier(random_state=42)
logistic_regression = LogisticRegression(random_state=42)

# Initialize imputer to handle missing values
imputer = SimpleImputer(strategy='mean')

# Impute missing values for training and test data
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Create a dictionary to store the classification reports and predictions
model_reports = {}

# Train and evaluate each model
print("Now.... RF")
accuracy, report, y_pred_rf = train_and_evaluate_model(random_forest, X_train_imputed, y_train, X_test_imputed, y_test, 'Random Forest')
model_reports['Random Forest'] = report

print("Now.... GB")
accuracy, report, y_pred_gb = train_and_evaluate_model(gradient_boosting, X_train_imputed, y_train, X_test_imputed, y_test, 'Gradient Boosting')
model_reports['Gradient Boosting'] = report

print("Now.... DT")
accuracy, report, y_pred_dt = train_and_evaluate_model(decision_tree, X_train_imputed, y_train, X_test_imputed, y_test, 'Decision Tree')
model_reports['Decision Tree'] = report

print("Now.... Logistic Regression")
accuracy, report, y_pred_lr = train_and_evaluate_model(logistic_regression, X_train_imputed, y_train, X_test_imputed, y_test, 'Logistic Regression')
model_reports['Logistic Regression'] = report

# Save the classification reports to a CSV file
reports_df = pd.DataFrame.from_dict(model_reports, orient='index', columns=['Classification Report'])
reports_df.to_csv('classification_reports.csv')

# Binarize the output labels for ROC curve calculation (multiclass case)
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])  # Assuming three classes (0, 1, 2); modify if necessary

# Initialize lists to store fpr, tpr, and roc_auc for each model
fpr_list = []
tpr_list = []
roc_auc_list = []

# Compute ROC curve and ROC AUC for each model (for each class)
for y_pred in [y_pred_lr, y_pred_dt, y_pred_rf, y_pred_gb]:
    fpr = {}
    tpr = {}
    roc_auc = {}

    # For each class, calculate ROC curve and AUC
    for i in range(y_test_bin.shape[1]):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred == i)
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Store fpr, tpr, and roc_auc
    fpr_list.append(fpr)
    tpr_list.append(tpr)
    roc_auc_list.append(roc_auc)

# Plot ROC curves for each model
plt.figure(figsize=(10, 8))
for i, roc_auc in enumerate(roc_auc_list):
    for j in roc_auc:
        plt.plot(fpr_list[i][j], tpr_list[i][j], lw=2, label=f'Model {i+1} Class {j} (AUC = {roc_auc[j]:0.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

"""#**Benchmark Model Selection and Hyperparameter Tuning**

**Logistic Regression is our Benchmark model**
"""

from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Handle missing values by imputing with the mean value
imputer = SimpleImputer(strategy='median')

# Impute the training and testing data
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Now define the hyperparameters for tuning using GridSearchCV
param_grid = {
    'penalty': ['l1', 'l2'],  # Regularization penalty
    'C': [0.001, 0.01, 0.1, 1, 4, 100],  # Inverse of regularization strength
    'solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem
}

# Initialize Logistic Regression Classifier
lr_classifier = LogisticRegression(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(lr_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the GridSearchCV to the imputed training data
grid_search.fit(X_train_imputed, y_train)

# Print the best hyperparameters found
print("Best Hyperparameters:", grid_search.best_params_)

# Use the best model found by GridSearchCV
best_lr_classifier = grid_search.best_estimator_

# Make predictions on the imputed test data using the best model
y_pred_lr_tuned = best_lr_classifier.predict(X_test_imputed)

# Calculate and print accuracy using the best model
accuracy_tuned = best_lr_classifier.score(X_test_imputed, y_test)
print("Tuned Model Accuracy:", accuracy_tuned)

# Generate a classification report using the tuned model
report_tuned = classification_report(y_test, y_pred_lr_tuned)
print("\nTuned Model Classification Report:\n", report_tuned)

# Calculate confusion matrix using the tuned model
cm_tuned = confusion_matrix(y_test, y_pred_lr_tuned)

# Plot confusion matrix using the tuned model
plt.figure(figsize=(8, 6))
sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix (Tuned Model)')
plt.show()

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values by imputing with the mean value
imputer = SimpleImputer(strategy='median')

# Impute the training and validation data
X_train_imputed = imputer.fit_transform(X_train)
X_val_imputed = imputer.transform(X_val)

# Train the model on the imputed training set
best_lr_classifier.fit(X_train_imputed, y_train)

# Predictions on the imputed training and validation sets
y_train_pred = best_lr_classifier.predict(X_train_imputed)
y_val_pred = best_lr_classifier.predict(X_val_imputed)

# Calculate accuracy on the training and validation sets
train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_val, y_val_pred)

# Print the accuracies
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

"""#**Pickle the Benchmark model**"""

import pickle

# Pickle the tuned logistic regression model
with open('tuned_logistic_regression_model.pkl', 'wb') as file:
    pickle.dump(best_lr_classifier, file)

# Pickle the Scaler model
with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

# Pickle the PCA model
with open('pca.pkl', 'wb') as file:
    pickle.dump(pca, file)